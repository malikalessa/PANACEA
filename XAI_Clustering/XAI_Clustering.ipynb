{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9c0342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import dalex as dx\n",
    "from keras.models import load_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca952417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_Dataset(): # To read the Original Dataset\n",
    "\n",
    "    path = './CICMalDroid2020/'\n",
    "    train_dataset  =  pd.read_csv(path +'train.csv')\n",
    "    test_dataset = pd.read_csv( path + 'test.csv')\n",
    "    y_train = train_dataset['Class']\n",
    "    y_test = test_dataset['Class']\n",
    "    try:\n",
    "        train_dataset.drop(['Class'], axis = 1, inplace=True)\n",
    "    except IOError:\n",
    "        print('IOERROR')\n",
    "    try:\n",
    "        test_dataset.drop(['Class'] , axis =1 , inplace= True)\n",
    "    except IOError:\n",
    "        print('IOERROR')\n",
    "\n",
    "    print('Train Dataset shape : ', train_dataset.shape)\n",
    "    print('Test Dataset shape : ', test_dataset.shape)\n",
    "    print('YTrain Dataset shape : ', y_train.shape)\n",
    "    print('Ytest Dataset shape : ', y_test.shape)\n",
    "    \n",
    "    print('y_train val_counts : ', y_train.value_counts())\n",
    "    print('y_test val_counts : ', y_test.value_counts())\n",
    "\n",
    "    return train_dataset, test_dataset, y_train,y_test\n",
    "\n",
    "\n",
    "def read_T_A_Datasets(train_dataset,y_train,dataset_no): # To read the Adversarial Samples and Add them to the \n",
    "    #original Dataset\n",
    "\n",
    "    path = './T_A/Datasets/'\n",
    "    \n",
    "    \n",
    "    columns = train_dataset.columns\n",
    "\n",
    "    train_adv_samples  =  pd.read_csv(path +'T_A_Dataset_MalDroid'+str(dataset_no)+'.csv')\n",
    "    \n",
    "    \n",
    "    y_train_adv = train_adv_samples['Class']\n",
    "\n",
    "    try:\n",
    "        train_adv_samples.drop(['Class'], axis = 1, inplace=True)\n",
    "    except IOError:\n",
    "        print('IOERROR')\n",
    "\n",
    "    print('Train Adv Dataset shape : ', train_adv_samples.shape)\n",
    "    print('YTrain Adv Dataset shape : ', y_train_adv.shape)\n",
    "    print('y_train adv val_counts : ', y_train_adv.value_counts())\n",
    "\n",
    "    train_dataset = train_dataset.append(train_adv_samples)\n",
    "    y_train = y_train.append(y_train_adv)\n",
    "\n",
    "    print('\\nTrain Dataset+Adversarial Samples shape : ', train_dataset.shape)\n",
    "    print('\\nYTrain Dataset+Adversarial Samples shape : ', y_train.shape)\n",
    "\n",
    "    train_dataset,y_train = shuffle(train_dataset,y_train, random_state=42)\n",
    "    \n",
    "    \n",
    "    return train_dataset, y_train\n",
    "\n",
    "train_dataset, test_dataset, y_train,y_test = read_Dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d56b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute the feature relevance in the models\n",
    "relevant_vector = []\n",
    "\n",
    "for i in range (100):\n",
    "    model = load_model('./Models/model_'+str(i)+'.h5')\n",
    "    dataset, y_dataset = read_T_A_Datasets(train_dataset,y_train,i)\n",
    "    dataset.reset_index(drop = True,inplace = True)\n",
    "    y_dataset.reset_index(drop = True,inplace = True)\n",
    "    print('shape ', dataset.shape)\n",
    "    print('shape', y_dataset.shape)\n",
    "    print('y_train : ', y_dataset.value_counts())\n",
    "    print('Counter : ', i)\n",
    "    explainer = dx.Explainer(model, dataset, y_dataset)\n",
    "    explanation = explainer.model_parts(random_state=42)\n",
    "    variable_importance = pd.DataFrame(explanation.result)\n",
    "    variable_importance = variable_importance.sort_values(by  = ['variable'], ascending= True)\n",
    "    \n",
    "    variable_importance.drop(['label'], axis=1, inplace = True)\n",
    "\n",
    "    relevant_vector.append(variable_importance)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faec455",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Relevance\n",
    "######### Append the Feature Relevance for the 100 models together #########################3\n",
    "\n",
    "df = pd.DataFrame(relevant_vector[0].set_index(['variable']), columns=['variable', 'dropout_loss'])\n",
    "df = df.rename(columns = {'dropout_loss':'0'})\n",
    "df.drop(['variable'], axis=1, inplace=True)\n",
    "\n",
    "df= df.transpose()\n",
    "\n",
    "for i in range(len(relevant_vector)):\n",
    "    if i==0:\n",
    "        continue\n",
    "    else:\n",
    "        df1 = pd.DataFrame(relevant_vector[i].set_index(['variable']), columns = ['variable', 'dropout_loss'])\n",
    "        df1.drop(['variable'], axis=1, inplace=True)\n",
    "        df1 = df1.rename(columns = {'dropout_loss':str(i)})\n",
    "        df1 = df1.transpose()\n",
    "        df = df.append(df1)\n",
    "    \n",
    "    \n",
    "dataset_path = 'C:/Users/Malik/PycharmProjects/Ensembled_for_Server/CIC_MalDroid_10%_470_features/Dalex/dalex_10%_100_CIC_MalDroid.csv'\n",
    "df.to_csv(path_or_buf = dataset_path, index=False)\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda596b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file\n",
    "path = 'C:/Users/Malik/PycharmProjects/Ensembled_for_Server/CIC_Maldroid_40_features/CIC_MalDroid2020_10%/Dalex/dalex_10%_100_CIC_MalDroid.csv'\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "df.drop(['_full_model_'],axis = 1, inplace = True)\n",
    "df.drop(['_baseline_'], axis = 1, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To the ELBOW\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "kmedoids = KMedoids( init='k-medoids++', method = 'pam',random_state= 42)\n",
    "\n",
    "visualizer = KElbowVisualizer(kmedoids, k=(100), metric='distortion')\n",
    "visualizer.fit(df)        # Fit the data to the visualizer\n",
    "visualizer.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ae52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ELBOW will be used here to create the clusters\n",
    "kmedoids = KMedoids(n_clusters = , init = 'k-medoids++', method = 'pam', random_state=42)#np.random.seed(seed))\n",
    "\n",
    "kmedoid = kmedoids.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Medoids\n",
    "\n",
    "medoid = kmedoid.medoid_indices_\n",
    "medoid.sort()\n",
    "print(medoid)\n",
    "print(kmedoid.inertia_)\n",
    "print(len(medoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5759808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to print the cluster's members\n",
    "re = kmedoid.predict(df)\n",
    "re\n",
    "\n",
    "clusters = []\n",
    "dictionary = {}\n",
    "n = 0\n",
    "for item in range(100):\n",
    "    #print(item)\n",
    "    if re[item] in dictionary:\n",
    "            value = []\n",
    "            value = dictionary[re[item]]\n",
    "            value.append(item)\n",
    "            dictionary[re[item]] = value\n",
    "        \n",
    "            #print(clusters[item])\n",
    "    else:\n",
    "            dictionary[re[item]] = [item]\n",
    "           # print(clusters[re[item]])\n",
    "\n",
    "sorted_clusters = sorted(dictionary.items(), key=lambda item: item[0])\n",
    "#print(sorted_clusters)\n",
    "\n",
    "for i in range(len(sorted_clusters)):\n",
    "    print(sorted_clusters[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
